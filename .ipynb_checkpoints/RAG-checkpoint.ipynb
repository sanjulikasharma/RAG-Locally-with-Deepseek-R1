{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain langchain-community faiss-cpu langchain-ollama python-dotenv docling langchain-docling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment setup\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "def load_and_convert_document(file_path):\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(file_path)\n",
    "    return result.document.export_to_markdown()\n",
    "\n",
    "source = \"C:/Users/sanju/Documents/DeepSeek R1-1.5/Google PDF.pdf\"\n",
    "markdown_content = load_and_convert_document(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mMarkdownHeaderTextSplitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mheaders_to_split_on\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'List[Tuple[str, str]]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mreturn_each_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstrip_headers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m      Splitting markdown files based on specified headers.\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Create a new MarkdownHeaderTextSplitter.\n",
      "\n",
      "Args:\n",
      "    headers_to_split_on: Headers we want to track\n",
      "    return_each_line: Return each line w/ associated headers\n",
      "    strip_headers: Strip split headers from the content of the chunk\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\sanju\\anaconda3\\lib\\site-packages\\langchain_text_splitters\\markdown.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "MarkdownHeaderTextSplitter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "def get_markdown_splits(markdown_content):\n",
    "    headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n",
    "    return markdown_splitter.split_text(markdown_content)\n",
    "\n",
    "\n",
    "chunks = get_markdown_splits(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and vector store setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "\n",
    "# Embedding and vector store setup\n",
    "def setup_vector_store(chunks):\n",
    "    embeddings = OllamaEmbeddings(model='nomic-embed-text', base_url=\"http://localhost:11434\")\n",
    "    single_vector = embeddings.embed_query(\"this is some text data\")\n",
    "    index = faiss.IndexFlatL2(len(single_vector))\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={}\n",
    "    )\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = setup_vector_store(chunks)\n",
    "# Setup retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159, 159)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index.ntotal, len(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke('what is revenue for september 2024?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def create_rag_chain(retriever):\n",
    "    prompt = \"\"\"\n",
    "        You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
    "        If you don't know the answer, just say that you don't know.\n",
    "        Answer in bullet points. Make sure your answer is relevant to the question and it is answered from the context only.\n",
    "        ### Question: {question} \n",
    "        \n",
    "        ### Context: {context} \n",
    "        \n",
    "        ### Answer:\n",
    "    \"\"\"\n",
    "    model = ChatOllama(model=\"deepseek-r1:1.5b\", base_url=\"http://localhost:11434\")\n",
    "    prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "\n",
    "    chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt_template\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Document \n",
    "source = \"C:/Users/sanju/Documents/DeepSeek R1-1.5/Google PDF.pdf\"\n",
    "markdown_content = load_and_convert_document(source)\n",
    "chunks = get_markdown_splits(markdown_content)\n",
    "\n",
    "# Create vector store \n",
    "vector_store = setup_vector_store(chunks)\n",
    "# Setup retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3})\n",
    "\n",
    "# RAG chain \n",
    "rag_chain = create_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much revenue is generated by Google?\n",
      "<think>\n",
      "Okay, so I need to figure out how much revenue Google makes based on the context provided. Let me read through it carefully.\n",
      "\n",
      "The context starts by talking about the \"Revenue Backlog\" which is $86.8 billion as of September 30, 2024. This backlog is from customer contracts that haven't been recognized yet for their services. The revenue is deferred because it's tied to when customers use Google's services.\n",
      "\n",
      "Next, there are details about Google's operations: they were incorporated in California and later moved to Delaware. In 2015, they reorganized as Alphabet Inc., which means Alphabet becomes the primary issuer now. They generate revenue through delivering online advertising, cloud-based solutions that provide infrastructure and platform services to enterprises, communication tools, and sales of other products like subscriptions, apps, in-app purchases, and devices.\n",
      "\n",
      "Hmm, but wait a minute. The context mentions \"revenue backlog\" specifically for Google Cloud and includes deferred revenue. It also notes that they expect half the backlog to be recognized over the next 24 months, with the rest likely to come later. But does this directly tell me how much actual revenue Google makes? I think not.\n",
      "\n",
      "The context doesn't provide a specific number; it's more about their plans and current state of operations rather than a particular revenue figure. It mentions that as of 2024, they have a backlog, but doesn't give an exact amount for the year or any other figures beyond that. So based on what I can gather from this context, I don't think there's enough information to provide a precise revenue figure for Google.\n",
      "\n",
      "I should make sure I'm not missing anything here. The context is about their current state and future expectations rather than a total revenue number. Therefore, the answer should mention that it's not possible to determine the exact revenue as of now without additional data.\n",
      "</think>\n",
      "\n",
      "- As of September 30, 2024, Google has a Revenue Backlog totaling $86.8 billion from customer contracts for future services.\n",
      "- This backlog includes deferred revenue and amounts that will be invoiced in future periods, excluding contracts with one-year or shorter terms and cancellable contracts.\n",
      "- The context does not provide an exact revenue figure for the year but focuses on their operational structure and planned revenue recognition timelines.\n",
      "- The number provided is a backlog amount, not a specific revenue total.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Query = \"How much revenue is generated by Google?\"\n",
    "\n",
    "print(Query)\n",
    "for chunks in rag_chain.stream(Query): \n",
    "    print(chunks, end='', flush= True)\n",
    "print('\\n'+ '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
